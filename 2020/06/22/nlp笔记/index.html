<!DOCTYPE html><html lang="en"><head><meta charset="utf-8"><meta name="X-UA-Compatible" content="IE=edge"><meta name="baidu-site-verification" content="1EB8XoOl0C"><meta name="google-site-verification" content="K7thEgdLm0UfRWJ5MGdF7sCcjClSzAlxFLPv2Oz5CGM"><title> ⭐️nlp笔记 · Hexo</title><meta name="viewport" content="width=device-width, initial-scale=1"><meta name="description" content="⭐️nlp笔记 - John Doe"><meta name="keywords"><meta name="author" content="John Doe"><link rel="short icon" href="https://bubuzou.oss-cn-shenzhen.aliyuncs.com/blog/202010/favicon.ico"><link rel="stylesheet" href="https://bubuzou.oss-cn-shenzhen.aliyuncs.com/blog/202010/bubuzou.css"><link rel="search" type="application/opensearchdescription+xml" href="http://example.com/atom.xml" title="Hexo"><script src="//cdn1.lncld.net/static/js/3.0.4/av-min.js"></script>
<script src='//unpkg.com/valine/dist/Valine.min.js'></script><meta name="generator" content="Hexo 5.4.0"><link rel="alternate" href="/atom.xml" title="Hexo" type="application/atom+xml">
</head><body><header><div class="header row"> <a href="/" class="logo-link"><img src="https://simg.nicepng.com/png/small/31-311776_royal…-free-collection-of-free-chipmunk-download-on.png"></a><ul id="nav_list" class="nav nav-list"><li class="nav-list-item"><a href="/" target="_self" data-hover="博文" class="nav-list-link">博文</a></li><li class="nav-list-item"><a href="/archives/" target="_self" data-hover="归档" class="nav-list-link">归档</a></li><li class="nav-list-item"><a href="/about/" target="_self" data-hover="关于" class="nav-list-link">关于</a></li></ul><div class="search"><a id="search_btn" href="#search"></a></div><div id="nav_btn" class="nav-btn"><span></span><span></span><span></span></div></div></header><div class="row scroll-con"><section class="container"><!-- for archive page--><div id="postAr" class="post"><article class="post-block"><h1 class="post-title">⭐️nlp笔记</h1><div class="post-info">2020-06-22</div><div class="post-content"><p>[TOC]</p>
<p><img src="https://tva1.sinaimg.cn/large/008i3skNly1gts7nenwaij61g90u041w02.jpg" alt="image-20210824214247284"></p>
<p>这是摘要……</p>
<span id="more"></span>

<h2 id="大框架"><a href="#大框架" class="headerlink" title="大框架"></a>大框架</h2><p>可以百度脑图查看，<a target="_blank" rel="noopener" href="https://naotu.baidu.com/file/f644044a8fb37fdba2d3d0bb4eb350e1?token=fd9855a9fc353aca">点击链接</a></p>
<p>出处：<a target="_blank" rel="noopener" href="https://github.com/NLP-LOVE/ML-NLP/blob/master/NLP/16.%20NLP/README.md">https://github.com/NLP-LOVE/ML-NLP/blob/master/NLP/16.%20NLP/README.md</a></p>
<p><img src="https://tva1.sinaimg.cn/large/007S8ZIlgy1gg1dm92gghj30wh0u0e83.jpg" alt="image-20200622211438309"></p>
<p><img src="https://tva1.sinaimg.cn/large/007S8ZIlgy1gg1dnz5umfj31f90u04qq.jpg" alt="image-20200622211617842"></p>
<p><img src="https://tva1.sinaimg.cn/large/007S8ZIlgy1gg1dow7xvdj31s80sgnpd.jpg" alt="image-20200622211710960"></p>
<img src="https://tva1.sinaimg.cn/large/008i3skNgy1gqzf10rubwj30u010xx6q.jpg" alt="img" style="zoom: 25%;" />

<p>链接：<a target="_blank" rel="noopener" href="https://blog.csdn.net/duan_zhihua/article/details/104069770">https://blog.csdn.net/duan_zhihua/article/details/104069770</a></p>
<h2 id="什么是nlp-？"><a href="#什么是nlp-？" class="headerlink" title="什么是nlp ？"></a>什么是nlp ？</h2><p><strong>自然语言处理</strong>（Natural Language Processing，NLP）是一门集语言学，数学及计算机科学于一体的科学。它的核心目标就是把人的自然语言转换为计算机可以阅读的指令，简单来说就是<strong>让机器读懂人的语言</strong>。</p>
<p>NLP是人工智能领域一个非常重要的分支，其它重要分支包括计算机视觉，语音及机器学习和深度学习等。那么，NLP与机器学习，深度学习有什么关系呢？我们可以用下面的图来表示。可以看出，深度学习是机器学习的其中一个分支，而自然语言处理与机器学习之间是并行的，机器学习为自然语言处理提供了解决问题的许多模型和方法。所以，二者之间具有密不可分的关系。</p>
<p><img src="https://tva1.sinaimg.cn/large/008eGmZEgy1gpijds0t37j30yy0i4qed.jpg" alt="image-20210413233114134"></p>
<ul>
<li>另外的一种理解</li>
</ul>
<p><a target="_blank" rel="noopener" href="https://easyai.tech/ai-definition/nlp/">https://easyai.tech/ai-definition/nlp/</a></p>
<p>不同物种都有自己的沟通方式</p>
<p>自然语言处理（NLP）就是在机器语言和人类语言之间沟通的桥梁，以实现人机交流的目的。</p>
<p><img src="https://tva1.sinaimg.cn/large/008eGmZEgy1gpikfgbwlij30ti0fegxc.jpg" alt="image-20210414000725230"></p>
<h2 id="语言模型"><a href="#语言模型" class="headerlink" title="语言模型"></a>语言模型</h2><p><a target="_blank" rel="noopener" href="https://zhuanlan.zhihu.com/p/334460660">如何通俗易懂地让女朋友明白什么是语言模型？</a></p>
<p><a target="_blank" rel="noopener" href="https://zhuanlan.zhihu.com/p/32292060">一起入门语言模型(Language Models)</a></p>
<p>对于语言序列 <img src="https://www.zhihu.com/equation?tex=w_1,w_2,...,w_n" alt="[公式]">，语言模型能计算该序列出现的概率，即</p>
<p>$$P(w_1,w_2,…,w_n)$$</p>
<p>等价于计算： $$P(w_i|w_1,w_2,…,w_{i-1})$$</p>
<p>换句话说， <strong>能够计算</strong> <img src="https://www.zhihu.com/equation?tex=p(w_%7Bi%7D%7Cw_%7B1%7D,...,w_%7Bi-1%7D)" alt="[公式]"> <strong>的模型就是语言模型</strong>。 </p>
<h3 id="n-gram-模型"><a href="#n-gram-模型" class="headerlink" title="n-gram 模型"></a>n-gram 模型</h3><p>是基于统计的语言模型，n代表考虑的前面词的相关的个数</p>
<p>n=1 unigram：<img src="https://www.zhihu.com/equation?tex=P(w_1,w_2,...,w_n)=%5Cprod_%7Bi=1%7D%5E%7Bn%7DP(w_i)" alt="[公式]"></p>
<p>n=2 bigram： <img src="https://www.zhihu.com/equation?tex=P(w_1,w_2,...,w_n)=%5Cprod_%7Bi=1%7D%5E%7Bn%7DP(w_i%7Cw_%7Bi-1%7D)" alt="[公式]"></p>
<p>n=3 trigram： <img src="https://www.zhihu.com/equation?tex=P(w_1,w_2,...,w_n)=%5Cprod_%7Bi=1%7D%5E%7Bn%7DP(w_i%7Cw_%7Bi-2%7D,w_%7Bi-1%7D)" alt="[公式]"></p>
<ul>
<li>复杂度：$|V|^n$</li>
</ul>
<p>直接这样计算会导致参数空间过大。</p>
<p>如，一个二元模型的参数计算结果如下</p>
<p><img src="https://tva1.sinaimg.cn/large/008i3skNgy1gr0hs5tbpmj30kh07a759.jpg" alt="640?wx_fmt=png"></p>
<blockquote>
<p>一个语言模型的参数就是所有的这些条件概率，试想按上面方式计算P(w 5 |w 1 ,w 2 ,w 3 ,w 4 )，这里<code>w_i</code>都有一个词典大小取值的可能，记作|V|，则该模型的参数个数是$|V|^5$，而且这还不包含P(w4 | w1, w2, w3)的个数，可以看到这样去计算条件概率会使语言模型参数个数过多而无法实用   —–<a target="_blank" rel="noopener" href="https://blog.csdn.net/qq_27825451/article/details/102457058">详解语言模型NGram及困惑度Perplexity</a></p>
</blockquote>
<h3 id="TF-IDF"><a href="#TF-IDF" class="headerlink" title="TF-IDF"></a>TF-IDF</h3><p>衡量词在当前文档中的重要程度，计算如下：</p>
<p><img src="https://latex.vimsky.com/test.image.latex.php?fmt=svg&val=%5Cdpi%7B150%7D%20%5Clarge%20TF-IDF(t,d)%20=%20TF(t,d)%5Ctimes%20IDF(t)&dl=0" alt="Latex公式渲染之后的图片"></p>
<p>TF为词t 在文档d 中的词频，IDF为逆文档频率</p>
<p><img src="https://www.zhihu.com/equation?tex=IDF+=+log(%5Cfrac%7B%E8%AF%AD%E6%96%99%E5%BA%93%E4%B8%AD%E6%96%87%E6%9C%AC%E7%9A%84%E6%80%BB%E4%B8%AA%E6%95%B0%7D%7B%E5%8C%85%E5%90%AB%E8%AF%A5%E8%AF%8D%E6%B1%87%E7%9A%84%E6%96%87%E6%9C%AC%E4%B8%AA%E6%95%B0+1%7D)" alt="[公式]"></p>
<ul>
<li>应用场景【1】<a name='toc1'></a></li>
</ul>
<p>自动提取关键词、信息检索、相似文章、自动摘要等</p>
<ol>
<li>自动提取关键词</li>
</ol>
<p>还是以《中国的蜜蜂养殖》为例，假定该文长度为1000个词，”中国”、”蜜蜂”、”养殖”各出现20次，则这三个词的”词频”（TF）都为0.02。然后，搜索Google发现，包含”的”字的网页共有250亿张，假定这就是中文网页总数。包含”中国”的网页共有62.3亿张，包含”蜜蜂”的网页为0.484亿张，包含”养殖”的网页为0.973亿张。则它们的逆文档频率（IDF）和TF-IDF如下：<img src="https://tva1.sinaimg.cn/large/008i3skNgy1gr0tq7m1bhj30ia07xq2u.jpg" alt="img"></p>
<p>tf-idf(‘蜜蜂’)=0.02*lg_10(250/0.484)=0.02 x 2.713=0.0543</p>
<p>从上表可见，”蜜蜂”的TF-IDF值最高，”养殖”其次，”中国”最低。（如果还计算”的”字的TF-IDF，那将是一个极其接近0的值。）所以，如果只选择一个词，”蜜蜂”就是这篇文章的关键词。</p>
<p>2）信息检索</p>
<p>信息检索要解决的问题是:给定一组关键字, 在所有的文档集合中, 返回与关键字相关的所有文档, 并对所有文档根据相关性进行排序.</p>
<p>信息检索时，对于每个文档，都可以分别计算一组搜索词（”中国”、”蜜蜂”、”养殖”）的TF-IDF，将它们相加，就可以得到整个文档的TF-IDF。这个值最高的文档就是与搜索词最相关的文档。</p>
<p>代码参考：<a target="_blank" rel="noopener" href="https://github.com/flitdu/consult/blob/main/tf_idf.py">https://github.com/flitdu/consult/blob/main/tf_idf.py</a></p>
<h3 id="FNN"><a href="#FNN" class="headerlink" title="FNN"></a>FNN</h3><p>Bengio 在03年的这篇经典 <a href="https://link.zhihu.com/?target=http://www.jmlr.org/papers/volume3/bengio03a/bengio03a.pdf">paper</a> 中，提出了如下图所示的前馈神经网络结构：</p>
<p><img src="https://tva1.sinaimg.cn/large/008i3skNgy1gr89swlnx2j30xc0syjtu.jpg" alt="img"></p>
<p>先给每个词在连续空间中赋予一个向量(词向量)，再通过神经网络去学习这种分布式表征。利用神经网络去建模当前词出现的概率与其前 n-1 个词之间的约束关系。很显然这种方式相比 n-gram 具有更好的<strong>泛化能力</strong>，<strong>只要词表征足够好。</strong>从而很大程度地降低了数据稀疏带来的问题。但是这个结构的明显缺点是仅包含了<strong>有限的前文信息</strong>。</p>
<h2 id="词向量"><a href="#词向量" class="headerlink" title="词向量"></a>词向量</h2><ul>
<li>Word2vec</li>
</ul>
<p>Word2Vec 是谷歌的分布式词向量工具。使用它可以很方便的得到词向量。Word2Vec 分别使用两个模型来学习词向量，一个是 CBOW(Continuous bag-of-word)，另一个是 Skip-gram模型。</p>
<p>CBOW 模型是指根据上下文来预测当前单词。而Skip-gram 是根据给定的词去预测上下文。所以这两个模型的本质是让模型去学习词和上下文的 co-occurrence。有意思的是，我们需要的词向量只是两个模型的“副产物”。</p>
<p><img src="https://tva1.sinaimg.cn/large/008i3skNgy1gs8r1lok98j319c0nw40t.jpg" alt="image-20210707222207677"></p>
<p>github实现： <a target="_blank" rel="noopener" href="https://github.com/chao-ji/tf-word2vec/blob/master/README.md">https://github.com/chao-ji/tf-word2vec/blob/master/README.md</a></p>
<ul>
<li>代码实现</li>
</ul>
<blockquote>
<p>Effectively, Word2Vec is based on distributional hypothesis where the context for each word is in its nearby words. Hence, by looking at its neighbouring words, we can attempt to predict the target word.</p>
<p>——–<a target="_blank" rel="noopener" href="https://towardsdatascience.com/an-implementation-guide-to-word2vec-using-numpy-and-google-sheets-13445eebd281">An implementation guide to Word2Vec using NumPy and Google Sheets: Learn the inner workings of Word2Vec</a></p>
<p>中译：<a target="_blank" rel="noopener" href="https://www.leiphone.com/news/201812/2o1E1Xh53PAfoXgD.html">手把手教你NumPy来实现Word2vec</a></p>
</blockquote>
<ul>
<li>CBOW 与skip-gram区别</li>
</ul>
<p>Skip-gram: works well with small amount of the training data, represents well even rare words or phrases<br>CBOW: several times faster to train than the skip-gram, slightly better accuracy for the frequent words</p>
<h3 id="skip-gram"><a href="#skip-gram" class="headerlink" title="skip-gram"></a>skip-gram</h3><blockquote>
<p><a target="_blank" rel="noopener" href="https://zhuanlan.zhihu.com/p/27234078">理解 Word2Vec 之 Skip-Gram 模型</a></p>
<p>里面介绍了负采样 的细节</p>
</blockquote>
<p>一般使用跳字模型的中心词向量作为词的表征向量。</p>
<ul>
<li>skip-gram 中概率乘积的理解【3】</li>
</ul>
<p>条件概率公式如下：</p>
<p><img src="https://tva1.sinaimg.cn/large/0081Kckwgy1gkgkdbzn13j30lx05hjse.jpg" alt="formula"></p>
<p>根据其神经网络结构图，可知两个向量其实分别对应于两个 不同矩阵W中的行和列</p>
<p><img src="https://tva1.sinaimg.cn/large/0081Kckwgy1gkgkdcndbvj30fx0a7jrv.jpg" alt="skip-gram"></p>
<blockquote>
<p>Skip-gram 损失函数</p>
</blockquote>
<p><img src="https://tva1.sinaimg.cn/large/0081Kckwgy1gkgkk1736nj314i0c8tmd.jpg" alt="image-20201107135928187"></p>
<p>参考：</p>
<p><a target="_blank" rel="noopener" href="https://www.zybuluo.com/Dounm/note/591752">Word2Vec-知其然知其所以然</a></p>
<p><a target="_blank" rel="noopener" href="https://www.cnblogs.com/peghoty/p/3857839.html">word2vec 中的数学原理详解</a></p>
<h2 id="FastText"><a href="#FastText" class="headerlink" title="FastText"></a>FastText</h2><p>fastText 是 2016年 FAIR(Facebook AIResearch) 推出的一款文本分类与向量化工具。</p>
<p>fastText 提出了子词嵌入（subword embedding）的方法，从而试图将构词信息引入 word2vec 中的 <strong>skip-gram</strong>。</p>
<p>比如，对于单词‘where’，当n = 3时，我们得到所有 ⻓度为3的子词:“&lt;wh”“whe”“her”“ere”“re&gt;”以及特殊子词“<where>”。将中心词向量表示成单词的子词向量之和。</p>
<blockquote>
<p>构词学（morphology）作为语言学的一个重要分支，研究的正是词的内部结构和形成方式。</p>
</blockquote>
<p>在 word2vec 中，我们并没有直接利用构词学中的信息。无论是在 <strong>skip-gram</strong> 还是 <strong>CBOW</strong> 中，我们将形态不同的单词用不同的向量来表示。例如，“dog”和“dogs”分别用两个不同的向量表示，而模型中并未直接表达这两个向量之间的关系。</p>
<ul>
<li><a target="_blank" rel="noopener" href="https://heleifz.github.io/14732610572844.html">fastText 源码分析</a></li>
</ul>
<p><img src="https://tva1.sinaimg.cn/large/0081Kckwgy1gkgv92i6kfj31ji0u0te3.jpg" alt="image-20200308224944226"></p>
<ul>
<li>FastText v0.1.0 中文文档</li>
</ul>
<p><a target="_blank" rel="noopener" href="https://www.bookstack.cn/books/fasttext-doc-zh">https://www.bookstack.cn/books/fasttext-doc-zh</a></p>
<h2 id="CNN"><a href="#CNN" class="headerlink" title="CNN"></a>CNN</h2><p><img src="https://tva1.sinaimg.cn/large/008i3skNly1gts6xc7p8lj60sg0q1tba02.jpg" alt="Illustration of a Convolutional Neural Network (CNN) architecture for sentence classification. Here we depict three filter region sizes: 2, 3 and 4, each of which has 2 filters. Every filter performs convolution on the sentence matrix and generates (variable-length) feature maps. Then 1-max pooling is performed over each map, i.e., the largest number from each feature map is recorded. Thus a univariate feature vector is generated from all six maps, and these 6 features are concatenated to form a feature vector for the penultimate layer. The final softmax layer then receives this feature vector as input and uses it to classify the sentence; here we assume binary classification and hence depict two possible output states. Source: hang, Y., &amp; Wallace, B. (2015). A Sensitivity Analysis of (and Practitioners’ Guide to) Convolutional Neural Networks for Sentence Classification"></p>
<h2 id="RNN"><a href="#RNN" class="headerlink" title="RNN"></a>RNN</h2><p>结构：<a target="_blank" rel="noopener" href="https://blog.csdn.net/m0_37788308/article/details/80632809">RNN（Recurrent Neural Network）循环神经网络</a></p>
<p><img src="https://tva1.sinaimg.cn/large/008i3skNgy1gstlc3g1stj31aq0i6jsr.jpg" alt="image-20210725230129775"></p>
<p><img src="https://tva1.sinaimg.cn/large/008i3skNgy1gtr3edhig4j60xo0aaq4502.jpg" alt="image-20210823223006553"></p>
<h2 id="相关文章"><a href="#相关文章" class="headerlink" title="相关文章"></a>相关文章</h2><table>
<thead>
<tr>
<th>标题</th>
<th>备注</th>
</tr>
</thead>
<tbody><tr>
<td><a target="_blank" rel="noopener" href="https://www.jiqizhixin.com/articles/2019-02-28-11">出身清华姚班，斯坦福博士毕业，她的毕业论文成了「爆款」</a></td>
<td></td>
</tr>
<tr>
<td><a target="_blank" rel="noopener" href="https://blog.csdn.net/GitChat/article/details/78087377">自然语言处理在电商的技术实践</a></td>
<td></td>
</tr>
<tr>
<td><a target="_blank" rel="noopener" href="https://zhuanlan.zhihu.com/p/58931044">解读NLP深度学习的各类模型</a></td>
<td><img src="https://tva1.sinaimg.cn/large/008i3skNgy1gstkvqd9n4j31dr0u0wkl.jpg" alt="image-20200917231113728"></td>
</tr>
</tbody></table>
<h2 id="参考"><a href="#参考" class="headerlink" title="参考"></a>参考</h2><ol>
<li><a target="_blank" rel="noopener" href="http://www.ruanyifeng.com/blog/2013/03/tf-idf.html">TF-IDF与余弦相似性的应用（一）：自动提取关键词</a><a href="#toc1">🔼</a></li>
<li><a target="_blank" rel="noopener" href="https://zhuanlan.zhihu.com/p/34219483">了解N-Gram模型</a></li>
<li><a target="_blank" rel="noopener" href="https://zhuanlan.zhihu.com/p/53425736">word2vec详解（CBOW，skip-gram，负采样，分层Softmax）</a></li>
<li><a target="_blank" rel="noopener" href="https://zhuanlan.zhihu.com/p/109954774">02. 语言模型（language Model）发展历史</a></li>
<li><a target="_blank" rel="noopener" href="https://zhuanlan.zhihu.com/p/52061158">深入理解语言模型 Language Model</a></li>
<li><a target="_blank" rel="noopener" href="https://kexue.fm/archives/6933">从语言模型到Seq2Seq：Transformer如戏，全靠Mask</a></li>
<li>论文：《A Survey of the Usages of Deep Learning for Natural Language Processing》</li>
<li><a target="_blank" rel="noopener" href="https://www.infoq.cn/article/qbloqm0rf*sv6v0jmulf">一文理解 Transformer 的工作原理</a></li>
<li></li>
</ol>
</div></article></div><div class="right-container"><div class="widget"><div id="arAnchorBar"></div></div></div></section></div><div class="right-menu"></div><div class="modal search-modal"><div class="input-field"><input type="text" id="search_input"><label for="search-input">搜索</label></div><div id="search_result" class="search-result"></div></div><div class="blog-overlay"></div><footer class="row"><div class="footer-con"><div class="paginator"><a href="/2020/07/25/%E7%AE%80%E5%8E%86%E6%80%9D%E8%80%83/" title="⭐️简历思考" class="prev">PREV</a><a href="/2020/06/15/Transformer%E4%B8%93%E9%A2%98/" title="⭐️Transformer专题" class="next">NEXT</a></div><div class="copyright"><p>© 2016 - 2021 <a target="_blank">John Doe</a>, powered by <a href="https://hexo.io/" target="_blank">Hexo</a> <br> and <a href="https://github.com/Bulandent/hexo-theme-bubuzou" target="_blank">hexo-theme-bubuzou</a></p><p> <span style="padding-right: 6px;">闽ICP备16007301号-2</span></p></div><div class="totop"><i></i></div></div></footer><script async src="//cdn.bootcss.com/mathjax/2.6.1/MathJax.js?config=TeX-MML-AM_CHTML"></script><script src="https://bubuzou.oss-cn-shenzhen.aliyuncs.com/blog/202010/jquery-1.8.2.min.js"></script><script src="https://bubuzou.oss-cn-shenzhen.aliyuncs.com/blog/202010/articleCatalog.js"></script><script src="https://bubuzou.oss-cn-shenzhen.aliyuncs.com/blog/202010/main.js"></script><script>(function(b,o,i,l,e,r){b.GoogleAnalyticsObject=l;b[l]||(b[l]=function(){(b[l].q=b[l].q||[]).push(arguments)});b[l].l=+new Date;e=o.createElement(i);r=o.getElementsByTagName(i)[0];e.src='//www.google-analytics.com/analytics.js';r.parentNode.insertBefore(e,r)}(window,document,'script','ga'));ga('create',"UA-65933410-1",'auto');ga('send','pageview');</script></body></html>