<!DOCTYPE html><html lang="en"><head><meta charset="utf-8"><meta name="X-UA-Compatible" content="IE=edge"><meta name="baidu-site-verification" content="1EB8XoOl0C"><meta name="google-site-verification" content="K7thEgdLm0UfRWJ5MGdF7sCcjClSzAlxFLPv2Oz5CGM"><title> ⭐️nlp笔记 · Hexo</title><meta name="viewport" content="width=device-width, initial-scale=1"><meta name="description" content="⭐️nlp笔记 - John Doe"><meta name="keywords"><meta name="author" content="John Doe"><link rel="short icon" href="https://bubuzou.oss-cn-shenzhen.aliyuncs.com/blog/202010/favicon.ico"><link rel="stylesheet" href="https://bubuzou.oss-cn-shenzhen.aliyuncs.com/blog/202010/bubuzou.css"><link rel="search" type="application/opensearchdescription+xml" href="http://example.com/atom.xml" title="Hexo"><script src="//cdn1.lncld.net/static/js/3.0.4/av-min.js"></script>
<script src='//unpkg.com/valine/dist/Valine.min.js'></script><meta name="generator" content="Hexo 5.4.0"><link rel="alternate" href="/atom.xml" title="Hexo" type="application/atom+xml">
</head><body><header><div class="header row"> <a href="/" class="logo-link"><img src="/images/logo.png"></a><ul id="nav_list" class="nav nav-list"><li class="nav-list-item"><a href="/" target="_self" data-hover="博文" class="nav-list-link">博文</a></li><li class="nav-list-item"><a href="/archives/" target="_self" data-hover="归档" class="nav-list-link">归档</a></li><li class="nav-list-item"><a href="/about/" target="_self" data-hover="关于" class="nav-list-link">关于</a></li></ul><div class="search"><a id="search_btn" href="#search"></a></div><div id="nav_btn" class="nav-btn"><span></span><span></span><span></span></div></div></header><div class="row scroll-con"><section class="container"><!-- for archive page--><div id="postAr" class="post"><article class="post-block"><h1 class="post-title">⭐️nlp笔记</h1><div class="post-info">2020-06-22</div><div class="post-content"><p>需要 改动：：：</p>
<p><img src="https://tva1.sinaimg.cn/large/008i3skNly1gts7nenwaij61g90u041w02.jpg" alt="image-20210824214247284"></p>
<span id="more"></span>

<h2 id="大框架"><a href="#大框架" class="headerlink" title="大框架"></a>大框架</h2><p>可以百度脑图查看，<a target="_blank" rel="noopener" href="https://naotu.baidu.com/file/f644044a8fb37fdba2d3d0bb4eb350e1?token=fd9855a9fc353aca">点击链接</a></p>
<p>出处：<a target="_blank" rel="noopener" href="https://github.com/NLP-LOVE/ML-NLP/blob/master/NLP/16.%20NLP/README.md">https://github.com/NLP-LOVE/ML-NLP/blob/master/NLP/16.%20NLP/README.md</a></p>
<p><img src="https://tva1.sinaimg.cn/large/007S8ZIlgy1gg1dm92gghj30wh0u0e83.jpg" alt="image-20200622211438309"></p>
<p><img src="https://tva1.sinaimg.cn/large/007S8ZIlgy1gg1dnz5umfj31f90u04qq.jpg" alt="image-20200622211617842"></p>
<p><img src="https://tva1.sinaimg.cn/large/007S8ZIlgy1gg1dow7xvdj31s80sgnpd.jpg" alt="image-20200622211710960"></p>
<img src="https://tva1.sinaimg.cn/large/008i3skNgy1gqzf10rubwj30u010xx6q.jpg" alt="img" style="zoom: 25%;">

<p>链接：<a target="_blank" rel="noopener" href="https://blog.csdn.net/duan_zhihua/article/details/104069770">https://blog.csdn.net/duan_zhihua/article/details/104069770</a></p>
<h2 id="什么是nlp-？"><a href="#什么是nlp-？" class="headerlink" title="什么是nlp ？"></a>什么是nlp ？</h2><p><strong>自然语言处理</strong>（Natural Language Processing，NLP）是一门集语言学，数学及计算机科学于一体的科学。它的核心目标就是把人的自然语言转换为计算机可以阅读的指令，简单来说就是<strong>让机器读懂人的语言</strong>。</p>
<ul>
<li><a target="_blank" rel="noopener" href="https://easyai.tech/ai-definition/nlp/">另外的一种理解</a></li>
</ul>
<p>不同物种都有自己的沟通方式</p>
<p>自然语言处理（NLP）就是在机器语言和人类语言之间沟通的桥梁，以实现人机交流的目的。</p>
<p><img src="https://tva1.sinaimg.cn/large/008eGmZEgy1gpikfgbwlij30ti0fegxc.jpg" alt="image-20210414000725230"></p>
<ul>
<li>发展脉络</li>
</ul>
<p><img src="https://tva1.sinaimg.cn/large/e6c9d24egy1h02wtsq3bpj20u009s0u1.jpg" alt="清华大学孙茂松：NLP 面临的三大真实挑战_人工智能_11"></p>
<p>Model sizes of language models from 2018–2020 (Credit: <a target="_blank" rel="noopener" href="https://www.stateof.ai/">State of AI Report 2020</a>)：</p>
<p><img src="https://tva1.sinaimg.cn/large/e6c9d24egy1h058mj920uj21fe0s8778.jpg" alt="img"></p>
<ul>
<li>补充</li>
</ul>
<p>🔥 <a target="_blank" rel="noopener" href="https://ruder.io/ml-highlights-2021/">ML and NLP Research Highlights of 2021</a></p>
<p><a target="_blank" rel="noopener" href="https://ruder.io/research-highlights-2020/">ML and NLP Research Highlights of 2020</a></p>
<h2 id="语言模型"><a href="#语言模型" class="headerlink" title="语言模型"></a>语言模型</h2><p><a target="_blank" rel="noopener" href="https://zhuanlan.zhihu.com/p/334460660">如何通俗易懂地让女朋友明白什么是语言模型？</a></p>
<p><a target="_blank" rel="noopener" href="https://zhuanlan.zhihu.com/p/32292060">一起入门语言模型(Language Models)</a></p>
<p>对于语言序列 <img src="https://www.zhihu.com/equation?tex=w_1,w_2,...,w_n" alt="[公式]">，语言模型能计算该序列出现的概率，即</p>
<p>$$P(w_1,w_2,…,w_n)$$</p>
<p>等价于计算： $$P(w_i|w_1,w_2,…,w_{i-1})$$</p>
<p>换句话说， <strong>能够计算</strong> <img src="https://www.zhihu.com/equation?tex=p(w_%7Bi%7D%7Cw_%7B1%7D,...,w_%7Bi-1%7D)" alt="[公式]"> <strong>的模型就是语言模型</strong>。 </p>
<p><strong>n-gram 模型</strong></p>
<p>是基于统计的语言模型，n代表考虑的前面词的相关的个数</p>
<p>n=1 unigram：<img src="https://www.zhihu.com/equation?tex=P(w_1,w_2,...,w_n)=%5Cprod_%7Bi=1%7D%5E%7Bn%7DP(w_i)" alt="[公式]"></p>
<p>n=2 bigram： <img src="https://www.zhihu.com/equation?tex=P(w_1,w_2,...,w_n)=%5Cprod_%7Bi=1%7D%5E%7Bn%7DP(w_i%7Cw_%7Bi-1%7D)" alt="[公式]"></p>
<p>n=3 trigram： <img src="https://www.zhihu.com/equation?tex=P(w_1,w_2,...,w_n)=%5Cprod_%7Bi=1%7D%5E%7Bn%7DP(w_i%7Cw_%7Bi-2%7D,w_%7Bi-1%7D)" alt="[公式]"></p>
<ul>
<li>复杂度：$|V|^n$</li>
</ul>
<p>直接这样计算会导致参数空间过大。</p>
<p>如，一个二元模型的参数计算结果如下</p>
<p><img src="https://tva1.sinaimg.cn/large/008i3skNgy1gr0hs5tbpmj30kh07a759.jpg" alt="640?wx_fmt=png"></p>
<blockquote>
<p>一个语言模型的参数就是所有的这些条件概率，试想按上面方式计算P(w 5 |w 1 ,w 2 ,w 3 ,w 4 )，这里<code>w_i</code>都有一个词典大小取值的可能，记作|V|，则该模型的参数个数是$|V|^5$，而且这还不包含P(w4 | w1, w2, w3)的个数，可以看到这样去计算条件概率会使语言模型参数个数过多而无法实用   —–<a target="_blank" rel="noopener" href="https://blog.csdn.net/qq_27825451/article/details/102457058">详解语言模型NGram及困惑度Perplexity</a></p>
</blockquote>
<p><strong>TF-IDF</strong></p>
<p>衡量词在当前文档中的重要程度，计算如下：</p>
<p><img src="https://latex.vimsky.com/test.image.latex.php?fmt=svg&val=%5Cdpi%7B150%7D%20%5Clarge%20TF-IDF(t,d)%20=%20TF(t,d)%5Ctimes%20IDF(t)&dl=0" alt="Latex公式渲染之后的图片"></p>
<p>TF为词t 在文档d 中的词频，IDF为逆文档频率</p>
<p><img src="https://www.zhihu.com/equation?tex=IDF+=+log(%5Cfrac%7B%E8%AF%AD%E6%96%99%E5%BA%93%E4%B8%AD%E6%96%87%E6%9C%AC%E7%9A%84%E6%80%BB%E4%B8%AA%E6%95%B0%7D%7B%E5%8C%85%E5%90%AB%E8%AF%A5%E8%AF%8D%E6%B1%87%E7%9A%84%E6%96%87%E6%9C%AC%E4%B8%AA%E6%95%B0+1%7D)" alt="[公式]"></p>
<ul>
<li>应用场景【1】<a name="toc1"></a></li>
</ul>
<p>自动提取关键词、信息检索、相似文章、自动摘要等</p>
<ol>
<li>自动提取关键词</li>
</ol>
<p>还是以《中国的蜜蜂养殖》为例，假定该文长度为1000个词，”中国”、”蜜蜂”、”养殖”各出现20次，则这三个词的”词频”（TF）都为0.02。然后，搜索Google发现，包含”的”字的网页共有250亿张，假定这就是中文网页总数。包含”中国”的网页共有62.3亿张，包含”蜜蜂”的网页为0.484亿张，包含”养殖”的网页为0.973亿张。则它们的逆文档频率（IDF）和TF-IDF如下：<img src="https://tva1.sinaimg.cn/large/008i3skNgy1gr0tq7m1bhj30ia07xq2u.jpg" alt="img"></p>
<p>tf-idf(‘蜜蜂’)=0.02*lg_10(250/0.484)=0.02 x 2.713=0.0543</p>
<p>从上表可见，”蜜蜂”的TF-IDF值最高，”养殖”其次，”中国”最低。（如果还计算”的”字的TF-IDF，那将是一个极其接近0的值。）所以，如果只选择一个词，”蜜蜂”就是这篇文章的关键词。</p>
<p>2）信息检索</p>
<p>信息检索要解决的问题是:给定一组关键字, 在所有的文档集合中, 返回与关键字相关的所有文档, 并对所有文档根据相关性进行排序.</p>
<p>信息检索时，对于每个文档，都可以分别计算一组搜索词（”中国”、”蜜蜂”、”养殖”）的TF-IDF，将它们相加，就可以得到整个文档的TF-IDF。这个值最高的文档就是与搜索词最相关的文档。</p>
<p>代码参考：<a target="_blank" rel="noopener" href="https://github.com/flitdu/consult/blob/main/tf_idf.py">https://github.com/flitdu/consult/blob/main/tf_idf.py</a></p>
<p><strong>FNN</strong></p>
<p>Bengio 在03年的这篇经典 <a href="https://link.zhihu.com/?target=http://www.jmlr.org/papers/volume3/bengio03a/bengio03a.pdf">paper</a> 中，提出了如下图所示的前馈神经网络结构：</p>
<p><img src="https://tva1.sinaimg.cn/large/008i3skNgy1gr89swlnx2j30xc0syjtu.jpg" alt="img"></p>
<p>先给每个词在连续空间中赋予一个向量(词向量)，再通过神经网络去学习这种分布式表征。利用神经网络去建模当前词出现的概率与其前 n-1 个词之间的约束关系。很显然这种方式相比 n-gram 具有更好的<strong>泛化能力</strong>，<strong>只要词表征足够好。</strong>从而很大程度地降低了数据稀疏带来的问题。但是这个结构的明显缺点是仅包含了<strong>有限的前文信息</strong>。</p>
<h2 id="词向量"><a href="#词向量" class="headerlink" title="词向量"></a>词向量</h2><p><strong>Word2vec</strong></p>
<p>Word2Vec 是谷歌的分布式词向量工具。使用它可以很方便的得到词向量。Word2Vec 分别使用两个模型来学习词向量，一个是 CBOW(Continuous bag-of-word)，另一个是 Skip-gram模型。</p>
<p>CBOW 模型是指根据上下文来预测当前单词。而Skip-gram 是根据给定的词去预测上下文。所以这两个模型的本质是让模型去学习词和上下文的 co-occurrence。有意思的是，我们需要的词向量只是两个模型的“副产物”。</p>
<blockquote>
<p>Effectively, Word2Vec is based on distributional hypothesis where the context for each word is in its nearby words. Hence, by looking at its neighbouring words, we can attempt to predict the target word.</p>
</blockquote>
<p><img src="https://tva1.sinaimg.cn/large/008i3skNgy1gs8r1lok98j319c0nw40t.jpg" alt="image-20210707222207677"></p>
<p>代码复现： </p>
<ul>
<li><p><a target="_blank" rel="noopener" href="https://github.com/chao-ji/tf-word2vec/blob/master/README.md">https://github.com/chao-ji/tf-word2vec/blob/master/README.md</a></p>
</li>
<li><p><a target="_blank" rel="noopener" href="https://towardsdatascience.com/an-implementation-guide-to-word2vec-using-numpy-and-google-sheets-13445eebd281">An implementation guide to Word2Vec using NumPy and Google Sheets: Learn the inner workings of Word2Vec</a></p>
</li>
</ul>
<p>​    中译：<a target="_blank" rel="noopener" href="https://www.leiphone.com/news/201812/2o1E1Xh53PAfoXgD.html">手把手教你NumPy来实现Word2vec**</a></p>
<p><strong>CBOW 与skip-gram区别</strong></p>
<p>Skip-gram: works well with small amount of the training data, represents well even rare words or phrases<br>CBOW: several times faster to train than the skip-gram, slightly better accuracy for the frequent words</p>
<p><strong>skip-gram</strong></p>
<p><a target="_blank" rel="noopener" href="https://zhuanlan.zhihu.com/p/27234078">理解 Word2Vec 之 Skip-Gram 模型</a></p>
<p>一般使用跳字模型的中心词向量作为词的表征向量。</p>
<ul>
<li>skip-gram 中概率乘积的理解【3】</li>
</ul>
<p>条件概率公式如下：</p>
<p><img src="https://tva1.sinaimg.cn/large/0081Kckwgy1gkgkdbzn13j30lx05hjse.jpg" alt="formula"></p>
<p>根据其神经网络结构图，可知两个向量其实分别对应于两个 不同矩阵W中的行和列</p>
<p><img src="https://tva1.sinaimg.cn/large/0081Kckwgy1gkgkdcndbvj30fx0a7jrv.jpg" alt="skip-gram"></p>
<ul>
<li>Skip-gram 损失函数</li>
</ul>
<p><img src="https://tva1.sinaimg.cn/large/0081Kckwgy1gkgkk1736nj314i0c8tmd.jpg" alt="image-20201107135928187"></p>
<p>参考：</p>
<p><a target="_blank" rel="noopener" href="https://www.zybuluo.com/Dounm/note/591752">Word2Vec-知其然知其所以然</a></p>
<p><a target="_blank" rel="noopener" href="https://www.cnblogs.com/peghoty/p/3857839.html">word2vec 中的数学原理详解</a></p>
<p><a target="_blank" rel="noopener" href="https://github.com/weberrr">weberrr</a>/<strong><a target="_blank" rel="noopener" href="https://github.com/weberrr/pytorch_word2vec">pytorch_word2vec</a></strong></p>
<h2 id="FastText"><a href="#FastText" class="headerlink" title="FastText"></a>FastText</h2><p>ft 严格上说，做了两件事：词向量和文本分类</p>
<p><strong>关于词向量</strong></p>
<p><a target="_blank" rel="noopener" href="https://www.connectedpapers.com/main/e2dba792360873aef125572812f3673b1a85d850/Enriching-Word-Vectors-with-Subword-Information/graph">原始论文</a>：</p>
<p><img src="https://tva1.sinaimg.cn/large/e6c9d24ely1h3pgit2dg7j21vi0qatfz.jpg" alt="image-20220629215623351"></p>
<p>在 word2vec 中，我们并没有直接利用构词学中的信息。无论是在 <strong>skip-gram</strong> 还是 <strong>CBOW</strong> 中，我们将形态不同的单词用不同的向量来表示。例如，“dog”和“dogs”分别用两个不同的向量表示，而模型中并未直接表达这两个向量之间的关系。</p>
<p>fastText 提出了子词嵌入（subword embedding）的方法，从而试图将构词信息引入 word2vec 中的 <strong>skip-gram</strong>。</p>
<p><del>比如，对于单词‘where’，当n = 3时，我们得到所有 ⻓度为3的子词:“&lt;wh”“whe”“her”“ere”“re&gt;”以及特殊子词“<where>”。将中心词向量表示成单词的子词向量之和。</where></del></p>
<blockquote>
<p>构词学（morphology）作为语言学的一个重要分支，研究的正是词的内部结构和形成方式。</p>
</blockquote>
<p><strong>关于 subwords 的计算</strong></p>
<p>如设<code>minn:3 maxn:3</code></p>
<p>则对于词 ‘here’ 和 ‘where’有：</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">&lt;here&gt;	计算Subwords: &lt;he, her, ere, re&gt;, </span><br><span class="line">	subwords[]存储（hash）: 33, 1188615, 1473455, 1529068, 867533, </span><br><span class="line">&lt;where&gt;	计算Subwords: &lt;wh, whe, her, ere, re&gt;, </span><br><span class="line">	subwords[]存储（hash）: 19, 167687, 420976, 1473455, 1529068, 867533, </span><br></pre></td></tr></table></figure>



<p><strong>关于文本分类</strong></p>
<p><a target="_blank" rel="noopener" href="https://www.connectedpapers.com/main/892e53fe5cd39f037cb2a961499f42f3002595dd/Bag-of-Tricks-for-Efficient-Text-Classification/graph">原始论文</a>：</p>
<img src="https://tva1.sinaimg.cn/large/e6c9d24ely1h3pg8qq6zfj21vm0qun4v.jpg" alt="image-20220629214642634" style="zoom: 50%;">

<p>fastText 是 2016年 FAIR(Facebook AIResearch) 推出的一款文本分类与向量化工具。</p>
<img src="https://tva1.sinaimg.cn/large/e6c9d24ely1h3pfwj1lftj20pu0doq3h.jpg" alt="image-20220629213453035" style="zoom: 50%;">

<p>其中，<code>inputs_</code>, <code>outputs_</code>为权重矩阵，定义参见：</p>
<figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">void</span> <span class="title">FastText::train</span><span class="params">(<span class="keyword">const</span> Args&amp; args, <span class="keyword">const</span> TrainCallback&amp; callback)</span> </span>&#123;</span><br><span class="line">	......</span><br><span class="line">  <span class="keyword">if</span> (!args_-&gt;pretrainedVectors.<span class="built_in">empty</span>()) &#123;</span><br><span class="line">    input_ = <span class="built_in">getInputMatrixFromFile</span>(args_-&gt;pretrainedVectors);</span><br><span class="line">  &#125; <span class="keyword">else</span> &#123;</span><br><span class="line">    input_ = <span class="built_in">createRandomMatrix</span>();</span><br><span class="line">  &#125;</span><br><span class="line">  output_ = <span class="built_in">createTrainOutputMatrix</span>();</span><br></pre></td></tr></table></figure>



<p><strong>一些疑问❓</strong></p>
<p>1）文本分类时，设置subword 参数后模型.bin变得很大</p>
<figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">./fasttext supervised -input train.txt -output model -epoch <span class="number">10</span> -minn <span class="number">1</span> -maxn <span class="number">3</span> -------&gt; <span class="number">800</span>MB</span><br></pre></td></tr></table></figure>

<p>vs </p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">./fasttext supervised -input train.txt -output model -epoch 10</span><br></pre></td></tr></table></figure>

<p>模型相差 数百倍</p>
<blockquote>
<p>回答：</p>
<p>主要是 bucket 默认大小  2000000，一旦设置’-minn 1 -maxn 3‘， bucket≠0， 将造成模型文件很大</p>
<p>模型 <code>.bin</code>文件大小与 dim 维度成正比，且与bucket 有关，input 初始化情况如下：</p>
</blockquote>
<figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">std::shared_ptr&lt;DenseMatrix&gt; input = std::make_shared&lt;DenseMatrix&gt;(</span><br><span class="line">  dict_-&gt;<span class="built_in">nwords</span>() + args_-&gt;bucket, args_-&gt;dim);</span><br></pre></td></tr></table></figure>

<p>2） 如何打印矩阵内容？</p>
<figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line"><span class="function">std::shared_ptr&lt;Matrix&gt; <span class="title">FastText::createRandomMatrix</span><span class="params">()</span> <span class="keyword">const</span> </span>&#123;</span><br><span class="line">  <span class="comment">// 如何打印矩阵内容呢？？</span></span><br><span class="line">  std::cout &lt;&lt; <span class="string">&quot;FastText::createRandomMatrix()------------&gt;&quot;</span> &lt;&lt; std::endl;</span><br><span class="line">  std::cout &lt;&lt; <span class="string">&quot;nwords: &quot;</span> &lt;&lt; dict_-&gt;<span class="built_in">nwords</span>() &lt;&lt; std::endl;</span><br><span class="line">  std::cout &lt;&lt; <span class="string">&quot;bucket: &quot;</span> &lt;&lt; args_-&gt;bucket &lt;&lt; std::endl;</span><br><span class="line">  std::cout &lt;&lt; <span class="string">&quot;dim: &quot;</span> &lt;&lt; args_-&gt;dim &lt;&lt; std::endl;</span><br><span class="line"></span><br><span class="line">  std::shared_ptr&lt;DenseMatrix&gt; input = std::make_shared&lt;DenseMatrix&gt;(</span><br><span class="line">      dict_-&gt;<span class="built_in">nwords</span>() + args_-&gt;bucket, args_-&gt;dim);</span><br><span class="line">  input-&gt;<span class="built_in">uniform</span>(<span class="number">1.0</span> / args_-&gt;dim, args_-&gt;thread, args_-&gt;seed);</span><br><span class="line"></span><br><span class="line">  std::cout &lt;&lt; <span class="string">&quot;FastText::createRandomMatrix()&lt;------------&quot;</span> &lt;&lt; std::endl;</span><br><span class="line">  <span class="keyword">return</span> input;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>



<p><strong>参考资料</strong></p>
<p><a target="_blank" rel="noopener" href="https://www.bookstack.cn/books/fasttext-doc-zh">FastText v0.1.0 中文文档</a></p>
<p><a target="_blank" rel="noopener" href="https://heleifz.github.io/14732610572844.html">fastText 源码分析</a></p>
<img src="https://tva1.sinaimg.cn/large/0081Kckwgy1gkgv92i6kfj31ji0u0te3.jpg" alt="image-20200308224944226" style="zoom:67%;">





<h2 id="CNN"><a href="#CNN" class="headerlink" title="CNN"></a>CNN</h2><p><img src="https://miro.medium.com/max/1400/0*0efgxnFIaLTZ2qkY" alt="img"></p>
<h2 id="RNN"><a href="#RNN" class="headerlink" title="RNN"></a>RNN</h2><p><strong><a target="_blank" rel="noopener" href="https://blog.csdn.net/m0_37788308/article/details/80632809">普通RNN</a></strong></p>
<img src="https://tva1.sinaimg.cn/large/008i3skNgy1gstlc3g1stj31aq0i6jsr.jpg" alt="image-20210725230129775" style="zoom:33%;">
$$
O_t =g(V \cdot S_t)\\
S_t = f(U \cdot X_t+W \cdot S_{t-1})
$$

<p><strong>LSTM</strong></p>
<p><img src="https://tva1.sinaimg.cn/large/008i3skNgy1gusyprm0l2j61q10nbgow02.jpg" alt="A LSTM neural network." style="zoom: 25%;"><img src="https://tva1.sinaimg.cn/large/008i3skNgy1gusypslygej60qe04xt8y02.jpg" alt="img" style="zoom:25%;"></p>
<p>from <a target="_blank" rel="noopener" href="http://colah.github.io/posts/2015-08-Understanding-LSTMs/">Understanding LSTM Networks</a></p>
<p>当然，LSTM 也可以堆叠增加深度，一般不会太深：</p>
<img src="https://tva1.sinaimg.cn/large/008i3skNgy1gusyk0mlz9j60j80fht9e02.jpg" alt="img">

<ul>
<li><p>双向</p>
<p><a target="_blank" rel="noopener" href="https://aclanthology.org/Q16-1026.pdf">https://aclanthology.org/Q16-1026.pdf</a></p>
</li>
</ul>
<img src="https://tva1.sinaimg.cn/large/008i3skNgy1gusztuwfmcj60ow0reaci02.jpg" alt="image-20210925171804749" style="zoom: 50%;">



<p><strong>seq2seq</strong></p>
<p>由2个rnn 组成，也称为encoder-decoder结构，可用于处理NLG问题，如机器翻译等</p>
<img src="https://tva1.sinaimg.cn/large/e6c9d24egy1h2135dvq0kj212s0joq4b.jpg" alt="image-20220508163944874" style="zoom: 33%;">

<p>但是，这里有一个基本假设：原始序列的最后一个h（上图红圈处）包含了该序列的去全部信息，实际上这很难做到，由此提出了 <u>注意力模型</u></p>
<p>注意力模型引入了encoder端更多的信息</p>
<blockquote>
<p>注意力机制早在上世纪九十年代就有研究，到2014年Volodymyr的《Recurrent Models of Visual Attention》一文中将其<strong>应用在视觉领域</strong>，后来伴随着2017年Ashish Vaswani的《Attention is all you need》中Transformer结构的提出，注意力机制在NLP,CV相关问题的网络设计上被广泛应用。</p>
</blockquote>
<h2 id="Transformer"><a href="#Transformer" class="headerlink" title="Transformer"></a>Transformer</h2><p>Youtube 讲解：<a target="_blank" rel="noopener" href="https://www.youtube.com/watch?v=TQQlZhbC5ps">Transformer Neural Networks - EXPLAINED! (Attention is all you need)</a></p>
<p>transformer整体结构如下：</p>
<p><img src="https://tva1.sinaimg.cn/large/e6c9d24egy1h17e2ru9jej20x40pedhy.jpg" alt="img"></p>
<p><strong>seq2seq</strong></p>
<p><a target="_blank" rel="noopener" href="https://zhuanlan.zhihu.com/p/147310766">整理了12小时，只为让你20分钟搞懂Seq2seq</a></p>
<p>Seq2seq 是一个Encoder-Decoder 结构的网络，它的输入是一个序列，输出也是一个序列。</p>
<p>Transformer 也是一个seq2seq的模型架构</p>
<p><u>Encoder 将可变长度的信号序列变为固定长度的向量表达，Decoder 将这个固定长度的向量变成可变长度的目标信号序列</u></p>
<p><img src="https://tva1.sinaimg.cn/large/008vxvgGgy1h867cww53wj30o008w74j.jpg" alt="img"></p>
<p>关于seq2seq，<strong>要区分训练和预测的不同</strong></p>
<ul>
<li>训练情况</li>
</ul>
<p><img src="https://tva1.sinaimg.cn/large/008vxvgGgy1h867cxpaadj31400izwgx.jpg" alt="img"></p>
<ul>
<li>预测</li>
</ul>
<p><img src="https://tva1.sinaimg.cn/large/008vxvgGgy1h867cx21ykj31400iq76y.jpg" alt="img"></p>
<p><strong>Attention机制</strong></p>
<p>自注意力机制可以保证词向量之间的足够交融</p>
<p><del><img src="https://tva1.sinaimg.cn/large/008vxvgGgy1h867h7w34hj30w60ekjs7.jpg" alt="img">优点：</del></p>
<ol>
<li><del>远距离相互依赖特征的捕获</del></li>
<li><del>并行化</del></li>
</ol>
<ul>
<li>self-attention 自注意力</li>
</ul>
<img src="https://tva1.sinaimg.cn/large/008i3skNgy1gtrjfqlvg7j613k0s6wi102.jpg" alt="image-20210824074502707" style="zoom:50%;">

<ul>
<li>多头机制</li>
</ul>
<img src="https://tva1.sinaimg.cn/large/008i3skNly1gtsbbc9ib8j616h0u0tcy02.jpg" alt="image-20210824234933682" style="zoom:50%;">

<p>设计特点：可以保证词向量之间的足够交融，也就是考虑上下文信息。也意味着词向量会更能体现具体的语境</p>
<p>注意，多头机制会对输入的原始向量进行reshape，产生(Q,K,V)</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Split the embedding into self.heads different pieces</span></span><br><span class="line">values = values.reshape(N, value_len, self.heads, self.head_dim)</span><br><span class="line">keys = keys.reshape(N, key_len, self.heads, self.head_dim)</span><br><span class="line">query = query.reshape(N, query_len, self.heads, self.head_dim)</span><br></pre></td></tr></table></figure>

<p><strong>Transformer结构</strong></p>
<p><a target="_blank" rel="noopener" href="https://lilianweng.github.io/posts/2018-06-24-attention/">Attention? Attention!</a> ⤵</p>
<p><img src="https://tva1.sinaimg.cn/large/008vxvgGgy1h867l4gy7ej31g40u042q.jpg" alt="img"></p>
<p>详细解读：</p>
<p><img src="https://tva1.sinaimg.cn/large/008vxvgGgy1h867lmn945j31400nd779.jpg" alt="img"></p>
<blockquote>
<p>上图整体上依然是个seq2seq结构，左边表示Encoder，右边表示Decoder，两边的N均表示将对应的block重复N次。</p>
<ol>
<li>首先将带有位置信息的Embedding形式的输入sequence <code>a</code>经过Multi-Head Attention处理成sequence <code>b</code>；</li>
<li>Add：将Multi-Head Attention的input <code>a</code>和output <code>b</code>相加得到<code>b&#39;</code>；</li>
<li>Norm：将<code>b&#39;</code>进行Layer Norm规范化处理；</li>
<li>为了增强模型表示能力，将上一步产生的结果递交给两层全连接的FFN，第一层ReLU激活，第二层线性激活，然后继续Add&amp;Norm处理；</li>
<li>将1-4步重复N次后进入Decoder，首先依然进行Multi-Head Attention处理，与上边不同的是加入Masked方法使得模型只能attend到已经产生的sequence，然后继续Add&amp;Norm处理；</li>
<li>将Encoder的结果进行Multi-Head Attention处理；</li>
<li>将上一步的结果进行FNN&amp;Add&amp;Norm操作；</li>
<li>将5-7步重复N次后得到最终output。</li>
</ol>
</blockquote>
<p>动图流程：</p>
<p><strong>强烈推荐：<a target="_blank" rel="noopener" href="https://zhuanlan.zhihu.com/p/98650532">Transformer原理：自底向上解析</a></strong></p>
<p><img src="https://tva1.sinaimg.cn/large/008vxvgGgy1h867neeejfg30hs0fq4qp.gif" alt="img"></p>
<p>参考：</p>
<ul>
<li><a target="_blank" rel="noopener" href="https://zhuanlan.zhihu.com/p/63191028">Transform详解(超详细) Attention is all you need论文</a></li>
</ul>
<p><img src="https://tva1.sinaimg.cn/large/008vxvgGgy1h867p532v0j31400n2gov.jpg" alt="img"></p>
<blockquote>
<p><a target="_blank" rel="noopener" href="https://zhuanlan.zhihu.com/p/113233908">batchNormalization与layerNormalization的区别</a>：</p>
<p><img src="https://tva1.sinaimg.cn/large/008vxvgGgy1h867p5e2scj30yq0jwjsw.jpg" alt="img"></p>
<p>Batch Normalization 的处理对象是对一批样本， Layer Normalization 的处理对象是单个样本。Batch Normalization 是对这批样本的同一维度特征做归一化， Layer Normalization 是对这单个样本的所有维度特征做归一化。</p>
</blockquote>
<ul>
<li>哈佛大学nlp组代码实现</li>
</ul>
<p>​        (1) Github <a target="_blank" rel="noopener" href="https://github.com/harvardnlp/annotated-transformer">https://github.com/harvardnlp/annotated-transformer</a></p>
<p>​        (2)  <a target="_blank" rel="noopener" href="http://nlp.seas.harvard.edu/2018/04/03/attention.html">http://nlp.seas.harvard.edu/2018/04/03/attention.html</a></p>
<img src="https://tva1.sinaimg.cn/large/008vxvgGgy1h867pry4fpj31i20s277b.jpg" alt="image-20220920170834250" style="zoom: 25%;">

<p>​        (3) <a target="_blank" rel="noopener" href="https://www.cnblogs.com/guoyaohua/p/transformer.html%E3%80%90%E8%AF%91%E6%96%87%E3%80%91">https://www.cnblogs.com/guoyaohua/p/transformer.html【译文】</a></p>
<h2 id="CNN-RNN-Trans比较"><a href="#CNN-RNN-Trans比较" class="headerlink" title="CNN/RNN/Trans比较"></a><strong>CNN/RNN/Trans比较</strong></h2><p><img src="https://tva1.sinaimg.cn/large/008vxvgGgy1h868qo1iaoj30yv0u0tbh.jpg" alt="image-20221115234303518"></p>
<p><img src="https://tva1.sinaimg.cn/large/008vxvgGgy1h868qy8bhcj316q0iyabp.jpg" alt="image-20221115234319495"></p>
<h2 id="预训练"><a href="#预训练" class="headerlink" title="预训练"></a>预训练</h2><p><strong>BERT</strong></p>
<img src="https://tva1.sinaimg.cn/large/008vxvgGgy1h867wi15pcj31240k2mzv.jpg" alt="image-20210211170514652" style="zoom:67%;">

<p><img src="https://tva1.sinaimg.cn/large/e6c9d24egy1h17ecwnlmmj21ap0u0wjb.jpg" alt="image-20200705000546978"></p>
<ul>
<li><p>代码解读：<a target="_blank" rel="noopener" href="https://hanielxx.com/MachineLearning/2021-02-23-bert-create-pretrain-data-analysis.html">BERT-预训练源码理解</a></p>
</li>
<li><p><a target="_blank" rel="noopener" href="https://zhuanlan.zhihu.com/p/103226488">BERT 详解</a></p>
</li>
</ul>
<p><strong>Fine-tune之后的NLP新范式：Prompt</strong></p>
<p>论文地址：<a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2107.13586.pdf">https://arxiv.org/pdf/2107.13586.pdf</a></p>
<p><img src="https://tva1.sinaimg.cn/large/e6c9d24egy1h057os796hj212407uwfl.jpg" alt="image-20210907225334299"></p>
<p><img src="https://tva1.sinaimg.cn/large/e6c9d24egy1h057p9lpw6j21ci0rg0xf.jpg" alt="ML and NLP Research Highlights of 2021"></p>
<h2 id="多模态"><a href="#多模态" class="headerlink" title="多模态"></a>多模态</h2><blockquote>
<p>颜水成博士从三个维度论证了多模态研究的重要性。</p>
<p>首先，观察自己女儿的学习过程，颜博士发现女儿明显通过图片、文字，声音等多模态形式的输入积累，知识才慢慢增长。与此同时，认识的文字，物品越来越多。因此，多模态相互作用，才能学得更好。既然人的学习是多模态共同的结果，机器学习应该也是一样的道理。</p>
<p>第二点，从人脑的一些研究表明，当人闭上眼睛只听声音时，视觉中枢神经元也会被激活；也就是说人脑中多模态在某些程度已经共享了一些东西了。</p>
<p>清华大学孙茂松：NLP 面临的三大真实挑战<br><a target="_blank" rel="noopener" href="https://blog.51cto.com/u_15282017/2971169">https://blog.51cto.com/u_15282017/2971169</a></p>
</blockquote>
<h2 id="部署"><a href="#部署" class="headerlink" title="部署"></a>部署</h2><ul>
<li> <a target="_blank" rel="noopener" href="https://github.com/DA-southampton/Tech_Aarticle">实战深度学习模型文章阅读积累，主要是搜广推，欢迎关注；</a></li>
</ul>
<blockquote>
<p>在我实际工作中，一般来说部署就是Flask+负载均衡，或者Grpc来提供服务。这个模块积累一下我看到不错的模型部署不错的文章</p>
<p><a target="_blank" rel="noopener" href="https://mp.weixin.qq.com/s/IzLtn1SR-aFuxXM3GNZbFw">蘑菇街自研服务框架如何提升在线推理效率？</a> 使用协程解决并发问题，使用FLask提供Restful接口，进行容器化部署 <a target="_blank" rel="noopener" href="https://zhuanlan.zhihu.com/p/77664408">如何解决推荐系统工程难题——深度学习推荐模型线上serving？</a> 介绍了几种serving方式，值得一看<br><a target="_blank" rel="noopener" href="https://zhuanlan.zhihu.com/p/61853955">爱奇艺基于CPU的深度学习推理服务优化实践-201904</a> 爱奇艺主要是在算法，应用以及系统三个方面对模型的部署进行优化。系统级主要是针对硬件平台上做的一些性能优化的方法，应用级是跟特定应用相关的分析以及优化的方法，算法级是针对算法的优化，例如模型的裁剪，模型的量化，在四个任务上提升了10倍左右（引自原文）</p>
</blockquote>
<h2 id="参考文章"><a href="#参考文章" class="headerlink" title="参考文章"></a>参考文章</h2><ol>
<li><a target="_blank" rel="noopener" href="http://www.ruanyifeng.com/blog/2013/03/tf-idf.html">TF-IDF与余弦相似性的应用（一）：自动提取关键词</a><a href="#toc1">🔼</a></li>
<li><a target="_blank" rel="noopener" href="https://zhuanlan.zhihu.com/p/34219483">了解N-Gram模型</a></li>
<li><a target="_blank" rel="noopener" href="https://zhuanlan.zhihu.com/p/53425736">word2vec详解（CBOW，skip-gram，负采样，分层Softmax）</a></li>
<li><a target="_blank" rel="noopener" href="https://zhuanlan.zhihu.com/p/109954774">02. 语言模型（language Model）发展历史</a></li>
<li><a target="_blank" rel="noopener" href="https://zhuanlan.zhihu.com/p/52061158">深入理解语言模型 Language Model</a></li>
<li><a target="_blank" rel="noopener" href="https://kexue.fm/archives/6933">从语言模型到Seq2Seq：Transformer如戏，全靠Mask</a></li>
<li><a target="_blank" rel="noopener" href="https://www.infoq.cn/article/qbloqm0rf*sv6v0jmulf">一文理解 Transformer 的工作原理</a></li>
<li><a target="_blank" rel="noopener" href="https://www.jiqizhixin.com/articles/2019-02-28-11">出身清华姚班，斯坦福博士毕业，她的毕业论文成了「爆款」</a></li>
<li><a target="_blank" rel="noopener" href="https://blog.csdn.net/GitChat/article/details/78087377">自然语言处理在电商的技术实践</a></li>
<li><a target="_blank" rel="noopener" href="https://zhuanlan.zhihu.com/p/58931044">解读NLP深度学习的各类模型</a></li>
<li><a target="_blank" rel="noopener" href="https://nlp.csai.tsinghua.edu.cn/news/tiaozhanbei/"><strong>基于图像识别技术的甲骨文数据系统</strong></a></li>
</ol>
<p><img src="https://tva1.sinaimg.cn/large/e6c9d24egy1h055z7jkexj20hs0b0js4.jpg" alt="挑战杯图片1.png"></p>
<ol start="12">
<li><a target="_blank" rel="noopener" href="https://www.6aiq.com/article/1587091834767"> PTMs：史上最全面总结 NLP 预训练模型</a></li>
</ol>
<img src="https://tva1.sinaimg.cn/large/008vxvgGgy1h867y13mfwj30u01bw0xd.jpg" alt="PTMs.jpg" style="zoom:50%;">

<ol start="13">
<li><a target="_blank" rel="noopener" href="https://mp.weixin.qq.com/s/Lo6tbA5lBgbtXFqatnq0pA">LSTM之父Jürgen Schmidhuber新作：一种方法，超越线性Transformers</a></li>
<li><a target="_blank" rel="noopener" href="https://mp.weixin.qq.com/s/S0dR--M88apEf5RV5dLmWw">「行知」NLP新星：BERT的优雅解读</a></li>
<li><a target="_blank" rel="noopener" href="https://www.cnblogs.com/sandwichnlp/p/11947627.html">预训练语言模型整理（ELMo/GPT/BERT…）</a></li>
<li><a target="_blank" rel="noopener" href="https://zhuanlan.zhihu.com/p/111513291">你保存的BERT模型为什么那么大？</a></li>
<li><a target="_blank" rel="noopener" href="https://zhuanlan.zhihu.com/p/86523925">学习AI之NLP后对预训练语言模型——心得体会总结</a></li>
<li><a target="_blank" rel="noopener" href="https://tech.meituan.com/2019/11/14/nlp-bert-practice.html">美团BERT的探索和实践</a></li>
<li></li>
</ol>
</div></article></div><div class="right-container"><div class="widget"><div id="arAnchorBar"></div></div><div class="widget"><div class="tagcloud"><h4>标签云</h4><a href="/tags/C/" style="font-size: 10px;">C</a> <a href="/tags/CS/" style="font-size: 15px;">CS</a> <a href="/tags/CSAPP/" style="font-size: 10px;">CSAPP</a> <a href="/tags/CV/" style="font-size: 10px;">CV</a> <a href="/tags/Django/" style="font-size: 10px;">Django</a> <a href="/tags/GPU/" style="font-size: 10px;">GPU</a> <a href="/tags/Git/" style="font-size: 10px;">Git</a> <a href="/tags/HTML/" style="font-size: 10px;">HTML</a> <a href="/tags/Hadoop/" style="font-size: 10px;">Hadoop</a> <a href="/tags/Python/" style="font-size: 15px;">Python</a> <a href="/tags/Pytorch/" style="font-size: 10px;">Pytorch</a> <a href="/tags/TensorFlow/" style="font-size: 10px;">TensorFlow</a> <a href="/tags/book/" style="font-size: 15px;">book</a> <a href="/tags/c/" style="font-size: 10px;">c++</a> <a href="/tags/flask/" style="font-size: 10px;">flask</a> <a href="/tags/linux/" style="font-size: 10px;">linux</a> <a href="/tags/lucene/" style="font-size: 10px;">lucene</a> <a href="/tags/mysql/" style="font-size: 10px;">mysql</a> <a href="/tags/nlp/" style="font-size: 15px;">nlp</a> <a href="/tags/shell/" style="font-size: 10px;">shell</a> <a href="/tags/%E4%BD%93%E7%B3%BB%E5%8C%96/" style="font-size: 10px;">体系化</a> <a href="/tags/%E5%85%89%E7%BA%BF%E8%BF%BD%E8%B8%AA/" style="font-size: 10px;">光线追踪</a> <a href="/tags/%E5%88%86%E8%AF%8D/" style="font-size: 10px;">分词</a> <a href="/tags/%E5%8A%9B%E6%89%A3/" style="font-size: 10px;">力扣</a> <a href="/tags/%E5%B7%A5%E5%85%B7/" style="font-size: 10px;">工具</a> <a href="/tags/%E5%B9%B6%E5%8F%91/" style="font-size: 10px;">并发</a> <a href="/tags/%E6%80%9D%E7%BB%B4/" style="font-size: 10px;">思维</a> <a href="/tags/%E6%8E%A8%E8%8D%90/" style="font-size: 10px;">推荐</a> <a href="/tags/%E6%90%9C%E7%B4%A2/" style="font-size: 15px;">搜索</a> <a href="/tags/%E6%91%98%E8%A6%81/" style="font-size: 10px;">摘要</a> <a href="/tags/%E6%95%B0%E5%AD%A6/" style="font-size: 10px;">数学</a> <a href="/tags/%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84/" style="font-size: 10px;">数据结构</a> <a href="/tags/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/" style="font-size: 15px;">机器学习</a> <a href="/tags/%E6%A0%91%E8%8E%93%E6%B4%BE/" style="font-size: 10px;">树莓派</a> <a href="/tags/%E6%A1%86%E6%9E%B6/" style="font-size: 10px;">框架</a> <a href="/tags/%E7%88%AC%E8%99%AB/" style="font-size: 15px;">爬虫</a> <a href="/tags/%E7%94%B5%E5%BD%B1/" style="font-size: 10px;">电影</a> <a href="/tags/%E7%9B%AE%E6%A0%87%E6%A3%80%E6%B5%8B/" style="font-size: 10px;">目标检测</a> <a href="/tags/%E7%A1%AC%E4%BB%B6/" style="font-size: 10px;">硬件</a> <a href="/tags/%E7%AE%80%E5%8E%86/" style="font-size: 10px;">简历</a> <a href="/tags/%E7%AE%97%E6%B3%95/" style="font-size: 20px;">算法</a> <a href="/tags/%E8%87%AA%E5%8A%A8%E9%A9%BE%E9%A9%B6/" style="font-size: 10px;">自动驾驶</a> <a href="/tags/%E8%87%AA%E5%AD%A6/" style="font-size: 10px;">自学</a> <a href="/tags/%E8%A7%84%E8%8C%83/" style="font-size: 10px;">规范</a> <a href="/tags/%E9%87%8F%E5%AD%90%E5%8A%9B%E5%AD%A6/" style="font-size: 10px;">量子力学</a></div></div></div></section></div><div class="right-menu"></div><div class="modal search-modal"><div class="input-field"><input type="text" id="search_input"><label for="search-input">搜索</label></div><div id="search_result" class="search-result"></div></div><div class="blog-overlay"></div><footer class="row"><div class="footer-con"><div class="paginator"><a href="/2020/07/25/%E7%AE%80%E5%8E%86%E6%80%9D%E8%80%83/" title="⭐️项目思考" class="prev">PREV</a><a href="/2020/05/24/%E3%80%90ML%E3%80%91%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%9F%A5%E8%AF%86%E7%82%B9/" title="⭐️机器学习知识点" class="next">NEXT</a></div><div class="copyright"><p>© 2016 - 2023 <a target="_blank">John Doe</a>, powered by <a href="https://hexo.io/" target="_blank">Hexo</a> <br> and <a href="https://github.com/Bulandent/hexo-theme-bubuzou" target="_blank">hexo-theme-bubuzou</a></p><p> <span style="padding-right: 6px;">闽ICP备16007301号-2</span></p></div><div class="totop"><i></i></div></div></footer><script async src="//cdn.bootcss.com/mathjax/2.6.1/MathJax.js?config=TeX-MML-AM_CHTML"></script><script src="https://bubuzou.oss-cn-shenzhen.aliyuncs.com/blog/202010/jquery-1.8.2.min.js"></script><script src="https://bubuzou.oss-cn-shenzhen.aliyuncs.com/blog/202010/articleCatalog.js"></script><script src="https://bubuzou.oss-cn-shenzhen.aliyuncs.com/blog/202010/main.js"></script><script>(function(b,o,i,l,e,r){b.GoogleAnalyticsObject=l;b[l]||(b[l]=function(){(b[l].q=b[l].q||[]).push(arguments)});b[l].l=+new Date;e=o.createElement(i);r=o.getElementsByTagName(i)[0];e.src='//www.google-analytics.com/analytics.js';r.parentNode.insertBefore(e,r)}(window,document,'script','ga'));ga('create',"UA-65933410-1",'auto');ga('send','pageview');</script></body></html>